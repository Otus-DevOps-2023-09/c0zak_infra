# c0zak_infra
c0zak Infra repository

-------------------------

ДЗ 9

Результаты работы
Общее ДЗ:
        Всё потыкано, все нужные файлы созданы.
        В связке с пакером пришлось запретить использование прокси, иначе не работал хендшейк, RSA/SHA-1 уже deprecated в OpenSSH, а прокся походу свежий билд юзает.
        Глянуть можно тут: https://github.com/hashicorp/packer-plugin-ansible/issues/69
        По итогу всё собралось и работает.
        Но тесты ясен пень это прожевать не могут, так что добавил корректные конфиги по соселству, а тесты пусть свой шит жуют.

Доп. задание:

        В ансибл добавлен плагин yc_compute.
        В каталоге создан динамический инвентори, yc_compute.yml
        Я немножко смухлевал, и в терраформе заменил теги, чтобы из них дёргать имена групп хостов))
        Проверил, всё работает на ура.

-------------------------

ДЗ 8

Результаты работы.
Общее ДЗ:
        Всё потыкано и попробовано.

Подпункт:
Теперь выполните ansible app -m command -a 'rm -rf ~/reddit'
и проверьте еще раз выполнение плейбука. Что изменилось и почему?
Добавьте информацию в README.md .

Ответ:
Изменилось поле changed, так как после сноса reddit для обеспечения идемпотентности пришлось его склонить заново, что обеспечило изменение конфигурации.

-------------------------

Доп. задание:
В описании многа букв.

По сути решение в создании скрипта, который сходит в s3, хапнет оттуда значения переменных с адресами хостов и на их основании запилит json инвентарь.
Собственно скрипт - generate.py
С целью полениться, он использует terraform чтобы сходить в s3, а не авторизуется сам. Да и как то нету официального плагина для ансибла у яндекса.
При вызове с параметром --list отдаёт в stdout динамический json инвентарь, ansible кушает и радуется.
Если не был инициализирован терраформ, или в стейте не нашлось адресов машин, то подставим 1.2.3.4 для группы app, и 4.3.2.1 для db.
--
Если вы разобрались с отличиями схем JSON для динамического и
статического инвентори, также добавьте описание в README

По сути отличия следующие:
        1) Статический внезапно... статичен, а динамический генерится на лету.
        2) Статический по сути - список хостов, каждый со своими переменными. Всё прибито гвоздями, хосты у нас стабильны. Хорошо подходит для bare-metal, где ничего годами не меняется. Динамический по сути список групп хостов, плюс отдельно вынесенные переменные. Ну и элементы групп как раз формируются динамически, на основании данных полученных откуда-нибудь, из того же облака.

-------------------------

ДЗ 7

Результаты работы.
Общее ДЗ:
        Созданы модули и конфиги stage и prod.

Перепишите блин свои ДЗ с чёткими описаниями тестов!
Откуда взялся storage-bucket.tf в них вообще? Не было такого требования. Скопипастил для теста, но вообще всё в каталоге storage.
Модуль vpc туда же.

-------------------------
Доп. задание
Задание со ⭐Задание со ⭐
1. Настройте хранение стейт файла в удаленном бекенде (remote
backends) для окружений stage и prod, используя Yandex Object
Storage в качестве бекенда. Описание бекенда нужно вынести в
отдельный файл backend.tf
2. Перенесите конфигурационные файлы Terraform в другую
директорию (вне репозитория). Проверьте, что state-файл
(terraform.tfstate) отсутствует. Запустите Terraform в обеих
директориях и проконтролируйте, что он "видит" текущее
состояние независимо от директории, в которой запускается
3. Попробуйте запустить применение конфигурации одновременно,
чтобы проверить работу блокировок
4. Добавьте описание в README.md

Решение
Изначально запилен бакет, создание описано в каталоге storage и модуле storage
Затем создано описание бэкенда. Минутка корявости - в версии terraform 1.6.3 захардкожено использование AWS при задании бэкенда типа s3.
Так что пришлось откатить на 1.5.7
Далее, для prod и stage инициализируется терраформ со статическими ключами к бэкенду
При выполнении развёртывания - всё отрабатывает, стейт складывает в s3.

-------------------------

Доп. задание
В процессе перехода от конфигурации, созданной в
предыдущем ДЗ к модулям мы перестали использовать provisioner
для деплоя приложения. Соответственно, инстансы поднимаются
без приложения.
1. Добавьте необходимые provisioner в модули для деплоя и работы
приложения. Файлы, используемые в provisioner, должны
находится в директории модуля.
2. Опционально можете реализовать отключение provisioner в
зависимости от значения переменной
3. Добавьте описание в README.md
P.S. Приложение получает адрес БД из переменной
окружения DATABASE_URL.

Решение
1) Для машины с приложением создан провижионер, в котором ставится reddit, плюс пробрасывается юнит, в который динамически сообщается
переменная DATABASE_URL, куда вписываем внутренний ip машины с монго. Для машины с БД динамически изменяется конфиг монго, чтобы
база слушала внутренний ip.
2) Т.к. терраформ не умеет в if-else, пришлось создавать динамический ресурс(null_resource), в рамках которого выполняются провижионеры для машины с приложением.
В зависимости от переменной need_deploy, у ресурса count равен 0 или 1. Костыль конечно, но по другому не придумалось.

Все изменения внесены в модули app и db соответственно.


-------------------------

ДЗ 6

Результаты работы.
Общее ДЗ:
        Созданы конфиги для терраформа, плюс каталог stubs где лежат заглушки файлов, для тестов.

-------------------------

Доп. задание:
Добавьте в код еще один terraform ресурс для нового инстанса
приложения, например reddit-app2, добавьте его в
балансировщик и проверьте, что при остановке на одном из
инстансов приложения (например systemctl stop puma),
приложение продолжает быть доступным по адресу
балансировщика; Добавьте в output переменные адрес второго
инстанса; Какие проблемы вы видите в такой конфигурации
приложения? Добавьте описание в README.md.

Решение:
Всё проверил, всё так и работает.
Проблема - килотонна конфига, запросто можно потеряться.

-------------------------

Доп. задание:
Как мы видим, подход с созданием доп. инстанса копированием
кода выглядит нерационально, т.к. копируется много кода.
Удалите описание reddit-app2 и попробуйте подход с заданием
количества инстансов через параметр ресурса count.
Переменная count должна задаваться в параметрах и по
умолчанию равна 1.

Решение:
Все конфиги в каталоге terraform/star_task
Решил делать по красоте :) Для начала, вместо базового образа использовал bake из прошлого дз. С целью того, чтобы ip наружу не торчал.
Далее, после создания ВМ из них тянутся внутренние ip адреса, конфиг locals.tf
И уже из них кастуется целевая группа, конфиг group.tf
На которую натравливается балансир, конфиг lb.tf
Проверил - всё работает, на отбой одной из вм реагирует сменой стауса балансира, но пока есть хоть одна вм - не падает.

-------------------------

ДЗ 5

Результаты работы.
Общее ДЗ:
	Каталог packer:
	ubuntu16.json - общее дз, использует параметры из variable.json (но параметры, как просили, в .gitignore)
	scripts/*.sh - общие скрипты запуска. Так как в облаке непонятные проблемы с использованием apt из пакера, пришлось добавить цикличный инсталл, пока не встанет.
	Создан дополнительно каталог stubs с файлом key.json, т.к. тест требует чтобы файл по вымышленному пути - существовал. Вы там в порядке вообще?)))
	Вот прям из теста:
	×  Command: `cd packer && packer validate -var-file=variables.json.example ubuntu16.json` stdout should eq "Template validated successfully.\n"
	expected: "Template validated successfully.\n"
        got: "Template validation failed. Errors are shown below.\n\nErrors validating build 'yandex'. 1 error(s) ...rvice account key file: key file 'SOME_PATH' read fail: open SOME_PATH: no such file or directory\n"

-------------------------

Дополнительное задание:
10.1*. Построение bake-образа (по10.1*. Построение bake-образа (по
желанию)желанию)
Чтобы попрактиковать подход к управлению инфраструктурой
Immutable infrastructure, о котором говорили на вебинаре,
попробуйте "запечь" (bake) в образ ВМ все зависимости
приложения и сам код приложения. Результат должен быть таким:
запускаем инстанс из созданного образа и на нем сразу же имеем
запущенное приложение.
Созданный шаблон должен называться immutable.json и
содержаться в директории packer, image_family у получившегося
образа должен быть reddit-full. Дополнительные файлы можно
положить в директорию packer/files. Для запуска приложения при
старте инстанса необходимо использовать systemd unit. Этот образ
можно строить как поверх нашего базового образа, так и поверх
стандартного образа ОС.


Решение:
В каталоге files лежат необходимые файлы, запуск через immutable.json, с использованием variable.json

-------------------------

Дополнительное задание:
Создайте скрипт create-reddit-vm.sh в директории config-
scripts, который будет создавать ВМ с помощью Yandex.Cloud CLI.
В скрипт не нужно включать инициализацию профиля в
Yandex.Cloud.

Скрипт лежит в корне репозитория/config-scripts/create-reddit-vm.sh
Использует изменённый metadata.yaml из каталога packer/files
Создаёт bake образ и запускает машину с ним. Использует переменную folder_id_var из файла variable.json при создании ВМ.

-------------------------

ДЗ 4

Дополнительное задание:
В качестве доп. задания используйте созданные ранее скрипты для создания startup
script , который будет запускаться при создании инстанса.
В результате применения данной команды CLI мы должны получать инстанс с уже
запущенным приложением. Startup скрипт необходимо закомитить, а используемую
команду CLI добавить в описание репозитория (README.md)

Решение:

В репе создан файл metadata.yaml, в котором описан процесс создания пользователя,
сам startup_script и его запуск.

CLI:

yc compute instance create \
 --name reddit-app \
 --hostname reddit-app \
 --memory=4 \
 --create-boot-disk image-folder-id=standard-images,image-family=ubuntu-1604-lts,size=10GB \
 --network-interface subnet-name=default-ru-central1-a,nat-ip-version=ipv4 \
 --zone ru-central1-a \
 --metadata serial-port-enable=1 \
 --metadata-from-file user-data=./metadata.yaml

-------------------------

testapp_IP = 158.160.40.104
testapp_port = 9292

-------------------------

ДЗ 3

Исследовать способ подключения к someinternalhost в одну команду из вашего
рабочего устройства, проверить работоспособность найденного решения и внести
его в README.md в вашем репозитории

Решение:

ssh -J bastion_IP someinternalhost

-------------------------

Дополнительное задание:
Предложить вариант решения для подключения из консоли при помощи команды
вида ssh someinternalhost из локальной консоли рабочего устройства, чтобы
подключение выполнялось по алиасу someinternalhost и внести его в README.md в
вашем репозитории


Решение:

Надо запилить конфиг, файл ~/.ssh/config

Добавить в него такие секции:

Host bastion
        Hostname bastion_IP
        User appuser
        IdentityFile ~/.ssh/appuser

Host someinternalhost
        Hostname someinternalhost_IP
        ProxyJump bastion
        User appuser
        IdentityFile ~/.ssh/appuser

После этого можно выполнить просто:
ssh someinternalhost

Кстати, после этого на бастионе можно также авторизоваться без указания ip:
ssh bastion

-------------------------

Дополнительное задание:
Сейчас веб-интерфейс VPN-сервера Pritunl работает с самоподписанным
сертификатом. И браузер постоянно ругается на это.
С помощью сервисов nip.io и реализуйте использование валидного сертификата для
панели управления VPNсервера

Решение:

Во первых вопрос, "С помощью сервисов nip.io и реализуйте" <- это что за чудное задание?))
Предположу что имелось ввиду использование сервиса Let's Encrypt

Решается посредством подстановки внешнего ip адреса vpn сервера с хвостом в виде nip.io
в конфигурацию Pritunl, что и было проделано.

-------------------------

bastion_IP = 84.201.130.36
someinternalhost_IP = 10.128.0.32

-------------------------
